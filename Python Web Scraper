# Scrape wrestling coach info 
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import time

# Base URL
url = "https://www.themat.com/membership/athletes/find-a-club"

# Initialize Selenium WebDriver
options = webdriver.ChromeOptions()
# Uncomment the following line to run in headless mode
options.add_argument('--headless')
driver = webdriver.Chrome(options=options)
wait = WebDriverWait(driver, 20)

# List to store state contact information
data = []

try:
    # Navigate to the main page
    driver.get(url)
    print("Page loaded. Switching to the iframe...")

    # Switch to the iframe
    iframe = wait.until(EC.presence_of_element_located((By.CLASS_NAME, "css-jt36ug")))
    driver.switch_to.frame(iframe)

    # Retrieve state links
    print("Extracting state links...")
    state_links = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "ul.states a")))
    state_links_data = [(state.text, state.get_attribute("href")) for state in state_links]
    print(f"Found {len(state_links_data)} states.")

    # Iterate through every state
    for i, (state_name, state_url) in enumerate(state_links_data, start=1):
        print(f"\nProcessing State {i}/{len(state_links_data)}: {state_name} ({state_url})")

        # Navigate to the state page
        driver.get(state_url)
        time.sleep(5)  # Allow time for the state page to load

        # Extract State Association Contact Information
        try:
            contact_info_section = wait.until(
                EC.presence_of_element_located((By.XPATH, "//h5[contains(text(), 'State Association Contact Information')]/.."))
            )
            # Remove the duplicate header text
            contact_info = "\n".join(contact_info_section.text.split("\n")[1:])
        except Exception:
            contact_info = "N/A"
            print(f"No State Association Contact Information found for {state_name}.")

        # Extract State Director Information
        try:
            state_director_section = driver.find_element(By.XPATH, "//h5[contains(text(), 'State Director')]/following-sibling::p")
            state_director_info = state_director_section.text
        except Exception:
            state_director_info = "N/A"
            print(f"No State Director Information found for {state_name}.")

        # Extract Membership Contact Information
        try:
            membership_contact_section = driver.find_element(By.XPATH, "//h5[contains(text(), 'Membership Contact')]/following-sibling::p")
            membership_contact_info = membership_contact_section.text
        except Exception:
            membership_contact_info = "N/A"
            print(f"No Membership Contact Information found for {state_name}.")

        # Extract SafeSport Coordinator Contact Information
        try:
            safesport_contact_section = driver.find_element(By.XPATH, "//h5[contains(text(), 'SafeSport Coordinator Contact')]/following-sibling::p")
            safesport_contact_info = safesport_contact_section.text
        except Exception:
            safesport_contact_info = "N/A"
            print(f"No SafeSport Coordinator Contact Information found for {state_name}.")

        # Append the data to the list
        data.append({
            "State": state_name,
            "State Association Contact": contact_info,
            "State Director": state_director_info,
            "Membership Contact": membership_contact_info,
            "SafeSport Coordinator Contact": safesport_contact_info,
        })

        # Navigate back to the main state list
        driver.get(url)
        time.sleep(5)  # Allow time for the main page to reload

        # Re-enter the iframe
        iframe = wait.until(EC.presence_of_element_located((By.CLASS_NAME, "css-jt36ug")))
        driver.switch_to.frame(iframe)

except Exception as e:
    print(f"An error occurred: {e}")

finally:
    # Quit the browser
    driver.quit()

    # Save the data to a CSV file
    if data:
        print("Saving data to 'state_contact_information.csv'...")
        df = pd.DataFrame(data)
        df.to_csv("state_contact_information.csv", index=False)
        print("Data saved successfully!")
    else:
        print("No data collected.")

# Scrape events from AYSO website
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import time

# Initialize Selenium WebDriver
options = webdriver.ChromeOptions()
# Uncomment the following line to run in headless mode
options.add_argument('--headless')
driver = webdriver.Chrome(options=options)

# Navigate to the website
url = "https://www.aysonational.org/Default.aspx?tabid=961582"
driver.get(url)

# Wait for the page to load
wait = WebDriverWait(driver, 20)

# Data storage for exporting to CSV
club_data = []

try:
    # Click the "Search" button
    print("Clicking the 'Search' button...")
    search_button = wait.until(EC.element_to_be_clickable((By.ID, "searchButton")))
    search_button.click()

    # Wait for results to load
    print("Waiting for results to load...")
    wait.until(EC.presence_of_element_located((By.ID, "locationList")))

    # Locate all club entries in the table
    print("Scraping club details...")
    clubs_table = wait.until(EC.presence_of_element_located((By.ID, "locationList")))
    club_rows = clubs_table.find_elements(By.TAG_NAME, "tr")  # Get all rows in the table

    # Initialize a set to track processed club names
    processed_clubs = set()

    # Iterate through each club entry
    for i, row in enumerate(club_rows, start=1):  # Adjust number of rows as needed
        try:
            # Extract the club name
            club_name = row.find_element(By.CLASS_NAME, "title-text").text if row.find_elements(By.CLASS_NAME, "title-text") else "N/A"

            # Get the count of entries for the club
            count = (
                int(row.find_element(By.CLASS_NAME, "count-text").text)
                if row.find_elements(By.CLASS_NAME, "count-text")
                else 1
            )

            # Skip if the club name is already processed or "N/A"
            if club_name in processed_clubs or club_name == "N/A":
                continue

            # Mark the club as processed
            processed_clubs.add(club_name)

            # Extract other details
            website = row.find_element(By.CLASS_NAME, "title-link").get_attribute("href") if row.find_elements(By.CLASS_NAME, "title-link") else "N/A"
            address = (
                row.find_element(By.XPATH, ".//div[@data-bind='html: Address']").get_attribute("innerHTML").replace("<br>", ", ")
                if row.find_elements(By.XPATH, ".//div[@data-bind='html: Address']")
                else "N/A"
            )
            email = (
                row.find_element(By.XPATH, ".//div[@data-bind='text: Email']").text
                if row.find_elements(By.XPATH, ".//div[@data-bind='text: Email']")
                else "N/A"
            )
            phone = (
                row.find_element(By.XPATH, ".//div[@data-bind='text: Phone']").text
                if row.find_elements(By.XPATH, ".//div[@data-bind='text: Phone']")
                else "N/A"
            )

            # Print details
            print(f"Club {len(processed_clubs)} (Count: {count}):")
            print(f"Name: {club_name}")
            print(f"Website: {website}")
            print(f"Address: {address}")
            print(f"Email: {email}")
            print(f"Phone: {phone}")
            print("-" * 40)

            # Add to the data list
            club_data.append({
                "Name": club_name,
                "Website": website,
                "Address": address,
                "Email": email,
                "Phone": phone
            })

        except Exception as e:
            print(f"Error extracting details for club {i}: {e}")

except Exception as e:
    print(f"An error occurred: {e}")

finally:
    # Quit the browser
    driver.quit()

# Export to CSV
if club_data:
    df = pd.DataFrame(club_data)
    output_file = "ayso_clubs.csv"
    df.to_csv(output_file, index=False)
    print(f"Data exported to '{output_file}'.")
else:
    print("No data to export.")

# Scrape Event URLs, Name, and Location from Raceroster website; Web scraper part 1
import requests
from tabulate import tabulate
import pandas as pd

# Base API URL
base_url = 'https://search.raceroster.com/search?q=5k&l=10&t=upcoming'

# HTTP headers
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',
}

# Placeholder for all events
all_events = []

# Initial API call to get the total number of events
response = requests.get(base_url + '&p=1', headers=headers)
if response.status_code != 200:
    print(f"Failed to fetch data. Status code: {response.status_code}")
    exit()

# Parse the response to find the total number of events
data = response.json()
total_events = data.get('meta', {}).get('hits', 0)
print(f"Total events found: {total_events}")

# Calculate total pages (10 events per page)
events_per_page = 10
total_pages = (total_events + events_per_page - 1) // events_per_page  # Ceiling division

# Paginate through all pages
for page in range(1, total_pages + 1):
    print(f"Fetching page {page}/{total_pages}...")
    response = requests.get(f"{base_url}&p={page}", headers=headers)
    if response.status_code != 200:
        print(f"Failed to fetch page {page}. Status code: {response.status_code}")
        continue

    # Extract event data
    events = response.json().get('data', [])
    for event in events:
        name = event.get("name", "N/A")
        url = event.get("url", "N/A")
        location = " ".join(
            [event["location"].get(key, "") or "" for key in ["address", "city", "country"] if event.get("location")]
        )
        all_events.append([name, url, location])

# Define table columns
columns = ["Name", "URL", "Location"]

# Print the table
print(tabulate(all_events, headers=columns))

# Save to CSV
df = pd.DataFrame(all_events, columns=columns)
df.to_csv('5k_events_paginated.csv', index=False, header=True)

print("Data saved to '5k_events_paginated.csv'")

# Save output from Web scraper part 2 to file
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

import pandas as pd

# Read data from the CSV file generated by scrape_event_contacts-5.py
data_file = '5k_events_paginated.csv'  # Ensure the path matches your file
df = pd.read_csv(data_file)
data = df[['Name', 'URL']].values.tolist()  # Convert to list of [title, url] pairs

def save_to_spreadsheet(data, output_file):
    if not data:
        print("No data to save.")
        return
    df = pd.DataFrame(data)
    df.to_excel(output_file, index=False)
    print(f"Data saved to {output_file}")


if __name__ == '__main__':
    url_counter = 0
    user_name = "gtrujillo"  # Edit to your PC User.
    data_length = len(data)

    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('--headless=new')
    browser = webdriver.Chrome(options=chrome_options)
    output_file = f"/Users/gtrujillo/Documents/event_contacts.xlsx"
    event_contacts = []

    for title, url in data:
        browser.get(url)
        try:
            wait = WebDriverWait(browser, 5)  # Adjust timeout if needed
            element = wait.until(EC.presence_of_element_located((By.ID, 'contact-information')))
            items = element.find_elements(By.CLASS_NAME, 'event-details__contact-list-item')
            contact, email, *args = items
            clean_name = contact.text.strip("Event contact ")
            clean_email = email.text.strip("Email ")
            print(clean_name, clean_email, title, url)
            event_contacts.append({
                "Event Name": title,
                "Event URL": url,
                "Event Contact": clean_name,
                "Email": clean_email
            })
        except Exception as e:
            print(e)
            print('No contact information extracted.', title, url)
            event_contacts.append({
                "Event Name": title,
                "Event URL": url,
                "Event Contact": '',
                "Email": ''
            })

        url_counter += 1

        print('Percentage: %i' % (url_counter / data_length * 100))

    browser.quit()
    print('Processed URL(s): %i.' % url_counter)

    save_to_spreadsheet(event_contacts, output_file)

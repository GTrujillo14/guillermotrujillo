# Scrape Event URLs, Name, and Location from Raceroster website; Web scraper part 1
import requests
from tabulate import tabulate
import pandas as pd

# Base API URL
base_url = 'https://search.raceroster.com/search?q=5k&l=10&t=upcoming'

# HTTP headers
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',
}

# Placeholder for all events
all_events = []

# Initial API call to get the total number of events
response = requests.get(base_url + '&p=1', headers=headers)
if response.status_code != 200:
    print(f"Failed to fetch data. Status code: {response.status_code}")
    exit()

# Parse the response to find the total number of events
data = response.json()
total_events = data.get('meta', {}).get('hits', 0)
print(f"Total events found: {total_events}")

# Calculate total pages (10 events per page)
events_per_page = 10
total_pages = (total_events + events_per_page - 1) // events_per_page  # Ceiling division

# Paginate through all pages
for page in range(1, total_pages + 1):
    print(f"Fetching page {page}/{total_pages}...")
    response = requests.get(f"{base_url}&p={page}", headers=headers)
    if response.status_code != 200:
        print(f"Failed to fetch page {page}. Status code: {response.status_code}")
        continue

    # Extract event data
    events = response.json().get('data', [])
    for event in events:
        name = event.get("name", "N/A")
        url = event.get("url", "N/A")
        location = " ".join(
            [event["location"].get(key, "") or "" for key in ["address", "city", "country"] if event.get("location")]
        )
        all_events.append([name, url, location])

# Define table columns
columns = ["Name", "URL", "Location"]

# Print the table
print(tabulate(all_events, headers=columns))

# Save to CSV
df = pd.DataFrame(all_events, columns=columns)
df.to_csv('5k_events_paginated.csv', index=False, header=True)

print("Data saved to '5k_events_paginated.csv'")

# Save output from Web scraper part 2 to file
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

import pandas as pd

# Read data from the CSV file generated by scrape_event_contacts-5.py
data_file = '5k_events_paginated.csv'  # Ensure the path matches your file
df = pd.read_csv(data_file)
data = df[['Name', 'URL']].values.tolist()  # Convert to list of [title, url] pairs

def save_to_spreadsheet(data, output_file):
    if not data:
        print("No data to save.")
        return
    df = pd.DataFrame(data)
    df.to_excel(output_file, index=False)
    print(f"Data saved to {output_file}")


if __name__ == '__main__':
    url_counter = 0
    user_name = "gtrujillo"  # Edit to your PC User.
    data_length = len(data)

    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('--headless=new')
    browser = webdriver.Chrome(options=chrome_options)
    output_file = f"/Users/gtrujillo/Documents/event_contacts.xlsx"
    event_contacts = []

    for title, url in data:
        browser.get(url)
        try:
            wait = WebDriverWait(browser, 5)  # Adjust timeout if needed
            element = wait.until(EC.presence_of_element_located((By.ID, 'contact-information')))
            items = element.find_elements(By.CLASS_NAME, 'event-details__contact-list-item')
            contact, email, *args = items
            clean_name = contact.text.strip("Event contact ")
            clean_email = email.text.strip("Email ")
            print(clean_name, clean_email, title, url)
            event_contacts.append({
                "Event Name": title,
                "Event URL": url,
                "Event Contact": clean_name,
                "Email": clean_email
            })
        except Exception as e:
            print(e)
            print('No contact information extracted.', title, url)
            event_contacts.append({
                "Event Name": title,
                "Event URL": url,
                "Event Contact": '',
                "Email": ''
            })

        url_counter += 1

        print('Percentage: %i' % (url_counter / data_length * 100))

    browser.quit()
    print('Processed URL(s): %i.' % url_counter)

    save_to_spreadsheet(event_contacts, output_file)
